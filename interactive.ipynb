{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinational Retail Data Centralisation\n",
    "\n",
    "This notebook is used to interactively work with the classes and the data returned so that development is easier. For example, interacting with the DataFrame to understand the data in the database, to create methods for cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "\n",
    "connector = DatabaseConnector()\n",
    "extractor = DataExtractor(connector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch DataFrame from table name\n",
    "\n",
    "Using connector to find table names, and then using extractor to produce a DataFrame of a specific table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector.list_db_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extractor.read_rds_table(\"legacy_users\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning user data\n",
    "\n",
    "Interactively attempting to clean the data in the user table, so that this can be implemented in the DataCleaning class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object columns to their respective type\n",
    "df = df.astype(\n",
    "    {\n",
    "        \"first_name\": \"string\",\n",
    "        \"last_name\": \"string\",\n",
    "        \"company\": \"string\",\n",
    "        \"email_address\": \"string\",\n",
    "        \"address\": \"string\",\n",
    "        \"country_code\": \"string\",\n",
    "        \"country\": \"string\",\n",
    "        \"user_uuid\": \"string\"\n",
    "    }\n",
    ")\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object date columns to the datetime type\n",
    "date_format = \"%Y-%m-%d\"\n",
    "df.date_of_birth = pd.to_datetime(df.date_of_birth, errors='coerce', format=date_format)\n",
    "df.join_date = pd.to_datetime(df.join_date, errors='coerce', format=date_format)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can confirm actual user entries among bad data by their UUID\n",
    "from re import search\n",
    "uuid_regex = r'^[0-9A-Za-z]{8}-[0-9A-Za-z]{4}-4[0-9A-Za-z]{3}-[89ABab][0-9A-Za-z]{3}-[0-9A-Za-z]{12}$'\n",
    "\n",
    "good_uuid = \"93caf182-e4e9-4c6e-bebb-60a1a9dcf9b8\"\n",
    "bad_uuid = \"AS45323\"\n",
    "\n",
    "match_good = search(uuid_regex, good_uuid)\n",
    "match_bad = search(uuid_regex, bad_uuid)\n",
    "match_good, match_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas suggest using pd.NA over numpy.nan for string type columns\n",
    "df.loc[~df.user_uuid.str.match(uuid_regex, na=False), 'user_uuid'] = pd.NA\n",
    "\n",
    "df[df.user_uuid.isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see some rows have incorrect country code GB as GGB\n",
    "df.country_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.country_code = df.country_code.replace(\"GGB\", \"GB\")\n",
    "df.country_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.phone_number.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phonenumbers\n",
    "import re\n",
    "\n",
    "def parse_phone_number(phone: str, region: str):\n",
    "    # Clean the phone number by removing (0), extensions, and other unnecessary characters\n",
    "    phone = re.sub(r'\\(0\\)', '', phone)  # Remove (0)\n",
    "    phone = phone.replace(\"(\", \"\").replace(\")\", \"\")  # Remove parentheses\n",
    "    phone = re.sub(r'x.*$', '', phone)  # Remove extensions (e.g., x1234)\n",
    "    phone = re.sub(r'[^\\d+]', '', phone)  # Remove non-numeric characters except for +\n",
    "\n",
    "    try:\n",
    "        # Attempt to parse the number with the phonenumbers library\n",
    "        # If no '+' sign, assume it's a local number and use the default region\n",
    "        if not phone.startswith('+'):\n",
    "            parsed_number = phonenumbers.parse(phone, region)\n",
    "        else:\n",
    "            parsed_number = phonenumbers.parse(phone)\n",
    "\n",
    "        # Format the parsed number in international format\n",
    "        return phonenumbers.format_number(parsed_number, phonenumbers.PhoneNumberFormat.INTERNATIONAL)\n",
    "\n",
    "    except phonenumbers.phonenumberutil.NumberParseException:\n",
    "        return None\n",
    "\n",
    "df.phone_number = df.apply(\n",
    "    lambda row: parse_phone_number(row['phone_number'], row['country_code']), axis=1\n",
    ") # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.country_code == \"DE\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any null rows\n",
    "df.replace(\"NULL\", pd.NA, inplace=True)\n",
    "df = df.dropna(how='any', axis='index')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning card data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PDF data as DataFrame\n",
    "pdf_dfs = extractor.retrieve_pdf_data(\"https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf\")\n",
    "\n",
    "pdf_dfs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some strange columns that got detected by tabular that need dropping\n",
    "cleaned_df = pdf_dfs.drop(columns=['card_number expiry_date', 'Unnamed: 0'])\n",
    "cleaned_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NULL with pandas na\n",
    "cleaned_df = cleaned_df.replace(\"NULL\", pd.NA)\n",
    "cleaned_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-numerical characters from card number\n",
    "cleaned_df.card_number = cleaned_df.card_number.replace(r'[^0-9]+', '', regex=True)\n",
    "cleaned_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty card numbers with pandas NA\n",
    "cleaned_df.card_number = cleaned_df.card_number.replace('', pd.NA, regex=True)\n",
    "cleaned_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert card number column to int, with coerce so any failed conversions are null\n",
    "cleaned_df.card_number = pd.to_numeric(cleaned_df.card_number, errors='coerce').astype(\"Int64\")\n",
    "cleaned_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change card_provider column to string\n",
    "cleaned_df.card_provider = cleaned_df.card_provider.astype(\"string\")\n",
    "cleaned_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert expiry date column to datetime\n",
    "cleaned_df.expiry_date = pd.to_datetime(cleaned_df.expiry_date, format=\"%m/%y\", errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API requests\n",
    "\n",
    "Using Json and requests lib to get data from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# retrieve_store_url, retrieve_store_count_url, header (x-api-key, Content-Type)\n",
    "api_config = extractor.load_api_config()\n",
    "api_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of stores from the API\n",
    "response = requests.get(api_config[\"retrieve_store_count_url\"], headers=api_config[\"header\"])\n",
    "data = response.json()\n",
    "data[\"number_stores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_store = api_config[\"retrieve_store_url\"] + str(0)\n",
    "response = requests.get(specific_store, headers=api_config[\"header\"])\n",
    "data = response.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "number_of_stores = 451\n",
    "store_jsons = []\n",
    "for i in range(number_of_stores):\n",
    "    specific_store = api_config[\"retrieve_store_url\"] + str(i)\n",
    "    response = requests.get(specific_store, headers=api_config[\"header\"])\n",
    "    store_jsons.append(response.json())\n",
    "    sleep(0.05) # rate limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(store_jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df = pd.DataFrame(store_jsons)\n",
    "store_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace N/A string with pandas NA\n",
    "cleaned_store_df = store_df.replace('N/A', pd.NA)\n",
    "cleaned_store_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace pythonic None from json response to pandas NA\n",
    "cleaned_store_df = cleaned_store_df.replace([None], pd.NA)\n",
    "cleaned_store_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noticed the lat column appears useless, since latitude exists with values. So drop it\n",
    "cleaned_store_df = cleaned_store_df.drop(columns=['lat'])\n",
    "cleaned_store_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change types\n",
    "cleaned_store_df = cleaned_store_df.astype(\n",
    "    {\n",
    "        \"address\": \"string\",\n",
    "        \"locality\": \"string\",\n",
    "        \"store_code\": \"string\",\n",
    "        \"store_type\": \"string\",\n",
    "        \"country_code\": \"string\",\n",
    "        \"continent\": \"string\"\n",
    "    }\n",
    ")\n",
    "cleaned_store_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove letters from any numbers in staff numbers\n",
    "cleaned_store_df.staff_numbers = cleaned_store_df.staff_numbers.replace(r'[^0-9]+', '', regex=True)\n",
    "cleaned_store_df.staff_numbers = cleaned_store_df.staff_numbers.replace('', pd.NA, regex=True)\n",
    "cleaned_store_df.staff_numbers = pd.to_numeric(cleaned_store_df.staff_numbers, errors='coerce').astype(\"Int64\")\n",
    "cleaned_store_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert long/latitude to float\n",
    "cleaned_store_df.longitude = pd.to_numeric(cleaned_store_df.longitude, errors='coerce').astype(\"float\")\n",
    "cleaned_store_df.latitude = pd.to_numeric(cleaned_store_df.latitude, errors='coerce').astype(\"float\")\n",
    "cleaned_store_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert opening date\n",
    "date_format = \"%Y-%m-%d\"\n",
    "cleaned_store_df.opening_date = pd.to_datetime(cleaned_store_df.opening_date, errors=\"coerce\", format=date_format)\n",
    "cleaned_store_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some strange values in here. We only want GB, DE, US\n",
    "cleaned_store_df.country_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only get rows whose country_code is a valid one\n",
    "country_codes = [\"GB\", \"DE\", \"US\"]\n",
    "mask = cleaned_store_df.country_code.isin(country_codes)\n",
    "cleaned_store_df = cleaned_store_df[mask]\n",
    "cleaned_store_df.country_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again some strange values here, time to replace\n",
    "cleaned_store_df.continent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_store_df.loc[:, 'continent'] = cleaned_store_df['continent'].str.replace(\"ee\", \"\")\n",
    "cleaned_store_df.continent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_store_df = cleaned_store_df.dropna(how='any', axis='index')\n",
    "cleaned_store_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_store_df.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
