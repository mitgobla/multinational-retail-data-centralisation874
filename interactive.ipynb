{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinational Retail Data Centralisation\n",
    "\n",
    "This notebook is used to interactively work with the classes and the data returned so that development is easier. For example, interacting with the DataFrame to understand the data in the database, to create methods for cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "\n",
    "connector = DatabaseConnector()\n",
    "extractor = DataExtractor(connector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch DataFrame from table name\n",
    "\n",
    "Using connector to find table names, and then using extractor to produce a DataFrame of a specific table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector.list_db_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extractor.read_rds_table(\"legacy_users\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning user data\n",
    "\n",
    "Interactively attempting to clean the data in the user table, so that this can be implemented in the DataCleaning class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object columns to their respective type\n",
    "df = df.astype(\n",
    "    {\n",
    "        \"first_name\": \"string\",\n",
    "        \"last_name\": \"string\",\n",
    "        \"company\": \"string\",\n",
    "        \"email_address\": \"string\",\n",
    "        \"address\": \"string\",\n",
    "        \"country_code\": \"string\",\n",
    "        \"country\": \"string\",\n",
    "        \"user_uuid\": \"string\"\n",
    "    }\n",
    ")\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object date columns to the datetime type\n",
    "date_format = \"%Y-%m-%d\"\n",
    "df.date_of_birth = pd.to_datetime(df.date_of_birth, errors='coerce', format=date_format)\n",
    "df.join_date = pd.to_datetime(df.join_date, errors='coerce', format=date_format)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can confirm actual user entries among bad data by their UUID\n",
    "from re import search\n",
    "uuid_regex = r'^[0-9A-Za-z]{8}-[0-9A-Za-z]{4}-4[0-9A-Za-z]{3}-[89ABab][0-9A-Za-z]{3}-[0-9A-Za-z]{12}$'\n",
    "\n",
    "good_uuid = \"93caf182-e4e9-4c6e-bebb-60a1a9dcf9b8\"\n",
    "bad_uuid = \"AS45323\"\n",
    "\n",
    "match_good = search(uuid_regex, good_uuid)\n",
    "match_bad = search(uuid_regex, bad_uuid)\n",
    "match_good, match_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas suggest using pd.NA over numpy.nan for string type columns\n",
    "df.loc[~df.user_uuid.str.match(uuid_regex, na=False), 'user_uuid'] = pd.NA\n",
    "\n",
    "df[df.user_uuid.isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see some rows have incorrect country code GB as GGB\n",
    "df.country_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.country_code = df.country_code.replace(\"GGB\", \"GB\")\n",
    "df.country_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.phone_number.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phonenumbers\n",
    "import re\n",
    "\n",
    "def parse_phone_number(phone: str, region: str):\n",
    "    # Clean the phone number by removing (0), extensions, and other unnecessary characters\n",
    "    phone = re.sub(r'\\(0\\)', '', phone)  # Remove (0)\n",
    "    phone = phone.replace(\"(\", \"\").replace(\")\", \"\")  # Remove parentheses\n",
    "    phone = re.sub(r'x.*$', '', phone)  # Remove extensions (e.g., x1234)\n",
    "    phone = re.sub(r'[^\\d+]', '', phone)  # Remove non-numeric characters except for +\n",
    "\n",
    "    try:\n",
    "        # Attempt to parse the number with the phonenumbers library\n",
    "        # If no '+' sign, assume it's a local number and use the default region\n",
    "        if not phone.startswith('+'):\n",
    "            parsed_number = phonenumbers.parse(phone, region)\n",
    "        else:\n",
    "            parsed_number = phonenumbers.parse(phone)\n",
    "\n",
    "        # Format the parsed number in international format\n",
    "        return phonenumbers.format_number(parsed_number, phonenumbers.PhoneNumberFormat.INTERNATIONAL)\n",
    "\n",
    "    except phonenumbers.phonenumberutil.NumberParseException:\n",
    "        return None\n",
    "\n",
    "df.phone_number = df.apply(\n",
    "    lambda row: parse_phone_number(row['phone_number'], row['country_code']), axis=1\n",
    ") # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.country_code == \"DE\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any null rows\n",
    "df.replace(\"NULL\", pd.NA, inplace=True)\n",
    "df = df.dropna(how='any', axis='index')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning card data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PDF data as DataFrame\n",
    "pdf_dfs = extractor.retrieve_pdf_data(\"https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf\")\n",
    "\n",
    "pdf_dfs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some strange columns that got detected by tabular that need dropping\n",
    "cleaned_df = pdf_dfs.drop(columns=['card_number expiry_date', 'Unnamed: 0'])\n",
    "cleaned_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NULL with pandas na\n",
    "cleaned_df = cleaned_df.replace(\"NULL\", pd.NA)\n",
    "cleaned_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-numerical characters from card number\n",
    "cleaned_df.card_number = cleaned_df.card_number.replace(r'[^0-9]+', '', regex=True)\n",
    "cleaned_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty card numbers with pandas NA\n",
    "cleaned_df.card_number = cleaned_df.card_number.replace('', pd.NA, regex=True)\n",
    "cleaned_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert card number column to int, with coerce so any failed conversions are null\n",
    "cleaned_df.card_number = pd.to_numeric(cleaned_df.card_number, errors='coerce').astype(\"Int64\")\n",
    "cleaned_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change card_provider column to string\n",
    "cleaned_df.card_provider = cleaned_df.card_provider.astype(\"string\")\n",
    "cleaned_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert expiry date column to datetime\n",
    "cleaned_df.expiry_date = pd.to_datetime(cleaned_df.expiry_date, format=\"%m/%y\", errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API requests\n",
    "\n",
    "Using Json and requests lib to get data from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# retrieve_store_url, retrieve_store_count_url, header (x-api-key, Content-Type)\n",
    "api_config = extractor.load_api_config()\n",
    "api_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of stores from the API\n",
    "response = requests.get(api_config[\"retrieve_store_count_url\"], headers=api_config[\"header\"])\n",
    "data = response.json()\n",
    "data[\"number_stores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_store = api_config[\"retrieve_store_url\"] + str(0)\n",
    "response = requests.get(specific_store, headers=api_config[\"header\"])\n",
    "data = response.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "number_of_stores = 451\n",
    "store_jsons = []\n",
    "for i in range(number_of_stores):\n",
    "    specific_store = api_config[\"retrieve_store_url\"] + str(i)\n",
    "    response = requests.get(specific_store, headers=api_config[\"header\"])\n",
    "    store_jsons.append(response.json())\n",
    "    sleep(0.05) # rate limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(store_jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df = pd.DataFrame(store_jsons)\n",
    "store_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace N/A string with pandas NA\n",
    "cleaned_store_df = store_df.replace('N/A', pd.NA)\n",
    "cleaned_store_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace pythonic None from json response to pandas NA\n",
    "cleaned_store_df = cleaned_store_df.replace([None], pd.NA)\n",
    "cleaned_store_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noticed the lat column appears useless, since latitude exists with values. So drop it\n",
    "cleaned_store_df = cleaned_store_df.drop(columns=['lat'])\n",
    "cleaned_store_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change types\n",
    "cleaned_store_df = cleaned_store_df.astype(\n",
    "    {\n",
    "        \"address\": \"string\",\n",
    "        \"locality\": \"string\",\n",
    "        \"store_code\": \"string\",\n",
    "        \"store_type\": \"string\",\n",
    "        \"country_code\": \"string\",\n",
    "        \"continent\": \"string\"\n",
    "    }\n",
    ")\n",
    "cleaned_store_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove letters from any numbers in staff numbers\n",
    "cleaned_store_df.staff_numbers = cleaned_store_df.staff_numbers.replace(r'[^0-9]+', '', regex=True)\n",
    "cleaned_store_df.staff_numbers = cleaned_store_df.staff_numbers.replace('', pd.NA, regex=True)\n",
    "cleaned_store_df.staff_numbers = pd.to_numeric(cleaned_store_df.staff_numbers, errors='coerce').astype(\"Int64\")\n",
    "cleaned_store_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert long/latitude to float\n",
    "cleaned_store_df.longitude = pd.to_numeric(cleaned_store_df.longitude, errors='coerce').astype(\"float\")\n",
    "cleaned_store_df.latitude = pd.to_numeric(cleaned_store_df.latitude, errors='coerce').astype(\"float\")\n",
    "cleaned_store_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert opening date\n",
    "date_format = \"%Y-%m-%d\"\n",
    "cleaned_store_df.opening_date = pd.to_datetime(cleaned_store_df.opening_date, errors=\"coerce\", format=date_format)\n",
    "cleaned_store_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some strange values in here. We only want GB, DE, US\n",
    "cleaned_store_df.country_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only get rows whose country_code is a valid one\n",
    "country_codes = [\"GB\", \"DE\", \"US\"]\n",
    "mask = cleaned_store_df.country_code.isin(country_codes)\n",
    "cleaned_store_df = cleaned_store_df[mask]\n",
    "cleaned_store_df.country_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again some strange values here, time to replace\n",
    "cleaned_store_df.continent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_store_df.loc[:, 'continent'] = cleaned_store_df['continent'].str.replace(\"ee\", \"\")\n",
    "cleaned_store_df.continent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_store_df = cleaned_store_df.dropna(how='any', axis='index')\n",
    "cleaned_store_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_store_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "url = \"s3://data-handling-public/products.csv\"\n",
    "bucket, file_key = url[5:].split('/', maxsplit=1)\n",
    "bucket, file_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = file_key.split('/')[-1] # if nested, this gets file from end\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "s3_client.download_file(bucket, file_key, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, \"r\") as csv_file:\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop additional index column\n",
    "cleaned_csv_data = data.drop(columns=['Unnamed: 0'])\n",
    "cleaned_csv_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Convert weights to kilogram across all values\n",
    "\n",
    "def convert_weight(weight: str) -> float | None:\n",
    "    # pattern creates two groups, one of which is the float value of the weight, the other being the unit\n",
    "    # ([\\d.]+) digit or a dot\n",
    "    # ([a-zA-Z]+) letters\n",
    "    if type(weight) is not str:\n",
    "        return\n",
    "\n",
    "    # Some weights have multipliers, like 12 x 250g\n",
    "    if \"x\" in weight:\n",
    "        multiplier, weight = weight.split(\"x\")\n",
    "        multiplier, weight = int(multiplier.strip()), weight.strip()\n",
    "    else:\n",
    "        multiplier = 1\n",
    "\n",
    "    pattern_matches = re.match(r'([\\d.]+)([a-zA-Z]+)', weight)\n",
    "\n",
    "    # No matches\n",
    "    if not pattern_matches:\n",
    "        return\n",
    "\n",
    "    # Try to convert\n",
    "    try:\n",
    "        value = float(pattern_matches.group(1))\n",
    "        unit = pattern_matches.group(2).lower()\n",
    "    except ValueError:\n",
    "        # value is not a float, so must be invalid\n",
    "        return\n",
    "\n",
    "    multiplied_value = multiplier * value\n",
    "\n",
    "    if unit == \"g\":\n",
    "        # Some gram values are meant to be kg, denoted by decimal\n",
    "        # i.e 1.2g should be 1.2kg\n",
    "        if not value.is_integer():\n",
    "            return multiplied_value\n",
    "        else:\n",
    "            return multiplied_value / 1000\n",
    "    elif unit == \"ml\":\n",
    "        return multiplied_value / 1000\n",
    "    elif unit == \"kg\":\n",
    "        # Already kilos\n",
    "        return multiplied_value\n",
    "    # Unknown unit or other\n",
    "    return\n",
    "\n",
    "convert_weight(\"1.52kg\"), convert_weight(\"123ml\"), convert_weight(\"1245g\"), convert_weight(\"10 x 125g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_csv_data[cleaned_csv_data.index == 1562]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_csv_data.weight = cleaned_csv_data.weight.apply(convert_weight)\n",
    "cleaned_csv_data.weight = pd.to_numeric(cleaned_csv_data.weight, errors='coerce').astype(\"float\")\n",
    "cleaned_csv_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check UUID format is correct\n",
    "cleaned_csv_data.loc[~cleaned_csv_data.uuid.str.match(uuid_regex, na=False), 'user_uuid'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strange values in removed column\n",
    "cleaned_csv_data.removed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only allow removed or still_available, null other options\n",
    "valid_removed_values = [\"Removed\", \"Still_avaliable\"]\n",
    "cleaned_csv_data.loc[~data.removed.isin(valid_removed_values)] = pd.NA\n",
    "cleaned_csv_data.removed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to boolean\n",
    "cleaned_csv_data.removed = cleaned_csv_data.removed.map({\"Removed\": True, \"Still_avaliable\": False}).astype(\"boolean\")\n",
    "cleaned_csv_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove £ from price\n",
    "cleaned_csv_data.product_price = cleaned_csv_data.product_price.str.removeprefix(\"£\")\n",
    "cleaned_csv_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert price to float\n",
    "cleaned_csv_data.product_price = cleaned_csv_data.product_price.astype(\"Float64\")\n",
    "cleaned_csv_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert other columns to string\n",
    "cleaned_csv_data = cleaned_csv_data.astype(\n",
    "    {\n",
    "        \"product_name\": \"string\",\n",
    "        \"category\": \"string\",\n",
    "        \"product_code\": \"string\",\n",
    "        \"uuid\": \"string\"\n",
    "    }\n",
    ")\n",
    "cleaned_csv_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert EAN to int\n",
    "cleaned_csv_data.EAN = pd.to_numeric(cleaned_csv_data.EAN, errors=\"coerce\").astype(\"int64\", errors=\"ignore\")\n",
    "cleaned_csv_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date_added to datetime\n",
    "cleaned_csv_data.date_added = pd.to_datetime(cleaned_csv_data.date_added, errors=\"coerce\", format=\"%Y-%m-%d\")\n",
    "cleaned_csv_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any NA values\n",
    "cleaned_csv_data = cleaned_csv_data.dropna(how=\"any\", axis=\"index\")\n",
    "cleaned_csv_data.head(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
